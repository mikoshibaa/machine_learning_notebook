# カーネルについてのメモ
カーネルについて個人的にかなり簡単にまとめてみた．

## 目的
モデルに教師データを用いてフィッティングしたときのウェイト $w$ を教師データを用いて明示的に表現する．新たな入力に対する出力は教師データの重み付けとして解釈できることを見る．

### 使う記号

* $\phi(x)$・・・入力に対する $M$ 次元特徴ベクトル． $\phi$ 一文字で特徴空間での入力と扱うことにする
* $\Phi$・・・ $N\times M $計画行列 (design matrix)
* $Y$・・・出力の教師データ $N$ 個を縦に並べた列ベクトル

## 前準備
モデルが多次元入力 $x$ から特徴量空間を介して重み付けされるとするモデルを組む

$$
\begin{align*}
    y &= \phi^T w
\end{align*}
$$

手元に教師データ $\{x_n,y_n\}$ があるとしてフィッティングさせる．損失基準は最小二乗誤差とする． 

## ウェイト $w$ の導出

最小二乗誤差でウェイト $w$ は次のように定まる．

$$
\begin{align*}
    w^* = (\Phi^T \Phi)^{-1} \Phi^T Y
\end{align*}
$$

$M\times M$行列 $S^{-1}=\sum_n \phi_n \phi^T = \Phi^T \Phi$ を用いると(相関行列に似ている)


$$
\begin{align*}
    w^* = S \Phi^T Y
\end{align*}
$$

とかける．


ウェイト $w$ が定まったので，教師データを捨てて新たな入力に対する出力は $y=\phi^T w*$ で終わりである．

しかし，ウェイトに教師データが吸収されているので，明示的に書き下すことで新たな入力と教師データの関係を見てみることにする．


## 新たな入力と教師データとの関係

新たな入力に対する出力 $y=\phi^T w^*$ を書き下してみる．

ポイントはモデルは $M$ 個のウェイトの足し算だったが，サンプルサイズ $N$ についての足し算と見ること．

$$
\begin{align*}
    y &= \phi^T w \\
    &= \phi^T S \Phi^T Y\\
    &= \phi^T S 
        \begin{pmatrix}
            \phi_1 & \phi_2 & \cdots & \phi_N
        \end{pmatrix}
        \begin{pmatrix}
            y_1 & y_2 & \cdots & y_N
        \end{pmatrix}^T\\
    &= \sum_{n} \phi^T S \phi_n y_n \\
    &=: \sum_{n} k(x,x_n) y_n
\end{align*}
$$

これは出力の教師データ $\{y_n\}$ の重み付き和である．その重みは新たな入力が教師データの入力と近いほど大きい．

この重み $k(x,x_n)$ を**カーネル**という．

## カーネル $k(x,x_n)$ についての解釈

上記の例でわかるようにカーネルは入力に対する内積 $<\phi, \phi_n>$ となっている．

$$
\begin{align*}
    k(x, x_n) = \phi(x)^T S \phi(x_n)
\end{align*}
$$

### カーネルから基底関数 $\phi_m(x)$ を作れる
カーネルの面白い点は入力 $x$ に対する基底関数 $\{\phi_m(x)\}$ を明示的に書かなくても，カーネルから基底関数が暗黙的に決まること