# 二値分類の識別モデルと生成モデルの関係

分類には大きく分けると以下の3つのアプローチがある

* 関数 $f(x)$ を直接作成する
* 識別モデル $p(c|x)$ を直接作成する
* 生成モデル $p(x)$　を導入して事後確率 $p(c|x)$　を考える

ここでは後ろの2つを取り上げ，互いに似ていることを見る．

## 識別モデル $p(c|x)$ を直接作成する

$x$ を重み付け和としてシグモイド関数にのせる．

$$
    c|x \sim Bernoulli(sigmoid(w^T x))\\
    \textbf{where } sigmoid(w^T x) = \frac{1}{1 + \exp(-w^T x)}
$$

識別面は原点を通り法線ベクトルが $w$ であるような超平面である( $\because w^T x = 0$ ).


## 生成モデル $p(x)$　を導入して事後確率 $p(c|x)$ を考える．

### ナイーブベイズモデル
クラスの条件付き確率をナイーブベイズモデルとする．

$$
    p(x|c) = \prod_{d = 1}^D \theta_{c,d}^{x_d}
$$

事後確率の式を変形するとシグモイド関数で表現できる．

$$
\begin{align*}
    p(c = 1|x) &= \frac{
        p(x|c = 1) p(c = 1)
    }{
        p(x|c = 0)p(c = 0) + 
        p(x|c = 1)p(c = 1)
    }\\
    &= 
    \frac{1}{
        1 + 
        \frac{p(x|c = 1)p(c = 1)}{p(x|c = 0)p(c = 0)}
    }\\
    &=
    \frac{1}{
        1 + 
        \exp[
            -\log \frac{p(c = 0)}{p(c = 1)}
            -\sum_{d} x_d \log \frac{\theta_{d,0}}{\theta_{d,1}}
        ]
    }\\
    &= \frac{1}{1 + \exp(-w^T x)}
\end{align*}
$$

### ガウス分布
クラスの条件付き確率をガウス分布とする．

$$
\begin{align*}
    p(x|c) &= \mathcal{N}(x|\mu_c,\Sigma)
\end{align*}
$$

事後確率はシグモイド関数で表現できる．
$$
\begin{align*}
    p(c = 1|x) &= \frac{p(x|c = 1)p(c = 1)}{p(x|c = 1)p(c = 1) + p(x|c = 0)p(c = 0)}\\
    &= \frac{1}{1 + \frac{p(x|c = 1)p(c = 1)}{p(x|c = 0)p(c = 0)}}
\end{align*}
$$

分母の1に足し算されてる項目は一次関数をエクスポネンシャルの中に入れたものなので，事後分布はシグモイド関数で表現できる．

## 識別モデルと生成モデルの違い

十分な量の教師データを使える場合，一般に識別モデルが優位．

### 識別モデル
* 教師データの**クラス境界**を直接学習
* 教師データの分類精度が高いクラス境界が得られる
* 外れ値に敏感

### 生成モデル
* クラスごとにデータの**確率分布**を学習
* 教師データの分類に最適なクラス境界を学習する保証なし
* 外れ値にロバスト